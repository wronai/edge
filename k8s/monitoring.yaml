---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    name: monitoring
---
# Prometheus RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
---
# Prometheus Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'edge-ai-cluster'
        environment: 'demo'
    
    rule_files:
      - "ai_alerts.yml"
    
    scrape_configs:
    # Prometheus self-monitoring
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
      scrape_interval: 30s
    
    # Kubernetes API server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https
    
    # Kubernetes nodes
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics
    
    # Kubernetes pods with prometheus annotations
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    # AI Services - ONNX & Ollama
    - job_name: 'ai-inference-services'
      static_configs:
      - targets: 
        - 'onnx-inference-svc.ai-inference:8001'
        - 'ollama-llm-svc.ai-inference:11435'
        - 'ai-gateway-svc.ai-inference:80'
      metrics_path: '/metrics'
      scrape_interval: 30s
      scrape_timeout: 10s
      
    # AI Gateway nginx metrics
    - job_name: 'nginx-gateway'
      static_configs:
      - targets: ['ai-gateway-svc.ai-inference:80']
      metrics_path: '/nginx-metrics'
      scrape_interval: 30s
  
  ai_alerts.yml: |
    groups:
    - name: ai-inference-alerts
      rules:
      # High latency alert
      - alert: AIHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="ai-inference-services"}[5m])) > 0.5
        for: 2m
        labels:
          severity: warning
          service: ai-inference
        annotations:
          summary: "High AI inference latency detected"
          description: "95th percentile latency is {{ $value }}s for {{ $labels.instance }}"
      
      # High error rate
      - alert: AIHighErrorRate
        expr: rate(http_requests_total{status=~"5..", job="ai-inference-services"}[5m]) > 0.1
        for: 1m
        labels:
          severity: critical
          service: ai-inference
        annotations:
          summary: "High error rate in AI inference"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.instance }}"
      
      # Pod restart alert
      - alert: AIPodRestarting
        expr: rate(kube_pod_container_status_restarts_total{namespace="ai-inference"}[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AI pod is restarting frequently"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting"
      
      # Resource usage alerts
      - alert: AIHighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{namespace="ai-inference"}[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage in AI services"
          description: "CPU usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"
      
      - alert: AIHighMemoryUsage
        expr: container_memory_usage_bytes{namespace="ai-inference"} / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in AI services"
          description: "Memory usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"
      
      # Service availability
      - alert: AIServiceDown
        expr: up{job="ai-inference-services"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "AI service is down"
          description: "AI service {{ $labels.instance }} is not responding"
---
# Prometheus Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.47.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus/'
          - '--web.console.libraries=/etc/prometheus/console_libraries'
          - '--web.console.templates=/etc/prometheus/consoles'
          - '--web.enable-lifecycle'
          - '--storage.tsdb.retention.time=15d'
          - '--storage.tsdb.wal-compression'
          - '--web.enable-admin-api'
        ports:
        - containerPort: 9090
          name: web
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
          readOnly: true
        - name: prometheus-storage
          mountPath: /prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 5
          periodSeconds: 5
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
          defaultMode: 0644
      - name: prometheus-storage
        emptyDir:
          sizeLimit: 10Gi
---
# Prometheus Service
apiVersion: v1
kind: Service
metadata:
  name: prometheus-svc
  namespace: monitoring
  labels:
    app: prometheus
spec:
  selector:
    app: prometheus
  ports:
  - name: web
    port: 9090
    targetPort: 9090
    nodePort: 30090
    protocol: TCP
  type: NodePort
---
# Grafana Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-provisioning
  namespace: monitoring
data:
  datasource.yml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus-svc:9090
      isDefault: true
      editable: true
  
  dashboard.yml: |
    apiVersion: 1
    providers:
    - name: 'ai-inference'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      updateIntervalSeconds: 10
      allowUiUpdates: true
      options:
        path: /var/lib/grafana/dashboards
---
# Grafana Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  ai-inference-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Edge AI Inference Platform",
        "tags": ["ai", "inference", "edge", "devops"],
        "style": "dark",
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "AI Service Status",
            "type": "stat",
            "targets": [
              {
                "expr": "up{job=\"ai-inference-services\"}",
                "legendFormat": "{{instance}}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "thresholds"},
                "thresholds": {
                  "steps": [
                    {"color": "red", "value": 0},
                    {"color": "green", "value": 1}
                  ]
                },
                "mappings": [
                  {"type": "value", "value": "0", "text": "DOWN"},
                  {"type": "value", "value": "1", "text": "UP"}
                ]
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Request Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{job=\"ai-inference-services\"}[5m]))",
                "legendFormat": "Total RPS"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "palette-classic"},
                "unit": "reqps"
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
          },
          {
            "id": 3,
            "title": "Response Time P95",
            "type": "stat",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"ai-inference-services\"}[5m]))",
                "legendFormat": "P95 Latency"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "thresholds"},
                "unit": "s",
                "thresholds": {
                  "steps": [
                    {"color": "green", "value": 0},
                    {"color": "yellow", "value": 0.1},
                    {"color": "red", "value": 0.5}
                  ]
                }
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
          },
          {
            "id": 4,
            "title": "Error Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status=~\"5..\",job=\"ai-inference-services\"}[5m])) / sum(rate(http_requests_total{job=\"ai-inference-services\"}[5m])) * 100",
                "legendFormat": "Error %"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "thresholds"},
                "unit": "percent",
                "thresholds": {
                  "steps": [
                    {"color": "green", "value": 0},
                    {"color": "yellow", "value": 1},
                    {"color": "red", "value": 5}
                  ]
                }
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0}
          },
          {
            "id": 5,
            "title": "Request Rate by Service",
            "type": "timeseries",
            "targets": [
              {
                "expr": "rate(http_requests_total{job=\"ai-inference-services\"}[5m])",
                "legendFormat": "{{instance}} - {{method}} {{status}}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "palette-classic"},
                "unit": "reqps"
              }
            },
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
          },
          {
            "id": 6,
            "title": "Response Time Distribution",
            "type": "timeseries",
            "targets": [
              {
                "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{job=\"ai-inference-services\"}[5m]))",
                "legendFormat": "P50"
              },
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"ai-inference-services\"}[5m]))",
                "legendFormat": "P95"
              },
              {
                "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{job=\"ai-inference-services\"}[5m]))",
                "legendFormat": "P99"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "palette-classic"},
                "unit": "s"
              }
            },
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
          },
          {
            "id": 7,
            "title": "CPU Usage",
            "type": "timeseries",
            "targets": [
              {
                "expr": "rate(container_cpu_usage_seconds_total{namespace=\"ai-inference\"}[5m])",
                "legendFormat": "{{pod}} - {{container}}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "palette-classic"},
                "unit": "percentunit"
              }
            },
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}
          },
          {
            "id": 8,
            "title": "Memory Usage",
            "type": "timeseries",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{namespace=\"ai-inference\"}",
                "legendFormat": "{{pod}} - {{container}}"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {"mode": "palette-classic"},
                "unit": "bytes"
              }
            },
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}
          },
          {
            "id": 9,
            "title": "Pod Status",
            "type": "table",
            "targets": [
              {
                "expr": "kube_pod_status_phase{namespace=\"ai-inference\"}",
                "legendFormat": "",
                "format": "table",
                "instant": true
              }
            ],
            "fieldConfig": {
              "defaults": {
                "custom": {
                  "displayMode": "color-background"
                }
              }
            },
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 24}
          }
        ],
        "time": {"from": "now-1h", "to": "now"},
        "refresh": "30s",
        "schemaVersion": 37,
        "version": 1
      }
    }
---
# Grafana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.1.0
        ports:
        - containerPort: 3000
          name: web
          protocol: TCP
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: "admin"
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin"
        - name: GF_USERS_ALLOW_SIGN_UP
          value: "false"
        - name: GF_INSTALL_PLUGINS
          value: "grafana-clock-panel,grafana-simple-json-datasource"
        - name: GF_PATHS_PROVISIONING
          value: "/etc/grafana/provisioning"
        volumeMounts:
        - name: grafana-provisioning
          mountPath: /etc/grafana/provisioning/datasources
          subPath: datasource.yml
          readOnly: true
        - name: grafana-provisioning
          mountPath: /etc/grafana/provisioning/dashboards
          subPath: dashboard.yml
          readOnly: true
        - name: grafana-dashboards
          mountPath: /var/lib/grafana/dashboards
          readOnly: true
        - name: grafana-storage
          mountPath: /var/lib/grafana
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        securityContext:
          runAsNonRoot: true
          runAsUser: 472
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
      volumes:
      - name: grafana-provisioning
        configMap:
          name: grafana-provisioning
          defaultMode: 0644
      - name: grafana-dashboards
        configMap:
          name: grafana-dashboards
          defaultMode: 0644
      - name: grafana-storage
        emptyDir:
          sizeLimit: 1Gi
---
# Grafana Service
apiVersion: v1
kind: Service
metadata:
  name: grafana-svc
  namespace: monitoring
  labels:
    app: grafana
spec:
  selector:
    app: grafana
  ports:
  - name: web
    port: 3000
    targetPort: 3000
    nodePort: 30030
    protocol: TCP
  type: NodePort
---
# AlertManager (opcjonalny - uproszczona wersja)
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@edge-ai.local'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://localhost:5001/alerts'
        send_resolved: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.26.0
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--web.external-url=http://localhost:9093'
        ports:
        - containerPort: 9093
          name: web
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
          readOnly: true
        - name: alertmanager-storage
          mountPath: /alertmanager
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config
          defaultMode: 0644
      - name: alertmanager-storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-svc
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  selector:
    app: alertmanager
  ports:
  - name: web
    port: 9093
    targetPort: 9093
    nodePort: 30093
  type: NodePort