
services:
  ollama:
    build:
      context: .
      dockerfile: docker/ollama/Dockerfile
    container_name: edge-ai-ollama
    ports:
      - "11435:11434"  # Map host port 11435 to container port 11434
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434  # Internal container port
      - OLLAMA_ORIGINS=*  # Allow all origins for testing
      - OLLAMA_NUM_PARALLEL=1  # Limit parallel processing
    restart: unless-stopped
    mem_limit: 8g
    mem_reservation: 4g
    memswap_limit: 12g
    cpus: 2.0
    # Remove the command to use the default entrypoint
    # The default entrypoint will start the Ollama server
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 120s

  onnx-runtime:
    image: mcr.microsoft.com/onnxruntime/server:latest
    container_name: edge-ai-onnx
    ports:
      - "8001:8001"
    environment:
      - ONNX_MODEL_PATH=/models
    command: ["--model_path", "/models", "--http_port", "8001"]
    volumes:
      - ./models:/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8001/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  prometheus:
    image: prom/prometheus:latest
    container_name: edge-ai-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: edge-ai-grafana
    ports:
      - "3007:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

  nginx-gateway:
    image: nginx:alpine
    container_name: edge-ai-gateway
    ports:
      - "30080:80"
    volumes:
      - ./configs/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      ollama:
        condition: service_healthy
      onnx-runtime:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3

volumes:
  ollama_data:
  grafana_data:
